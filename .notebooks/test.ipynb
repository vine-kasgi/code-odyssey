{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e3dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+------------+-------------+\n",
      "|cust_id|order_id|order_date|order_status|next_odr_date|\n",
      "+-------+--------+----------+------------+-------------+\n",
      "|      1|     101|2023-01-01|           Y|   2023-01-25|\n",
      "|      1|     102|2023-01-25|           Y|   2023-03-01|\n",
      "|      1|     103|2023-03-01|           Y|         NULL|\n",
      "|      2|     201|2023-01-10|           Y|   2023-02-25|\n",
      "|      2|     203|2023-02-25|           Y|         NULL|\n",
      "|      3|     301|2023-02-01|           Y|   2023-02-25|\n",
      "|      3|     302|2023-02-25|           Y|         NULL|\n",
      "+-------+--------+----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "from pyspark.sql.functions import col, lead, datediff, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 101, date(2023, 1, 1), 'Y'),\n",
    "    (1, 102, date(2023, 1, 25), 'Y'),   # within 30 days of previous\n",
    "    (1, 103, date(2023, 3, 1), 'Y'),    # >30 days from 1/25\n",
    "    (2, 201, date(2023, 1, 10), 'Y'),\n",
    "    (2, 202, date(2023, 2, 15), 'N'),\n",
    "    (2, 203, date(2023, 2, 25), 'Y'),   # >30 days from 1/10\n",
    "    (3, 301, date(2023, 2, 1), 'Y'),\n",
    "    (3, 302, date(2023, 2, 25), 'Y'),   # within 30 days\n",
    "    (4, 401, date(2023, 1, 1), 'N'),\n",
    "    (4, 402, date(2023, 2, 1), 'N'),    # no successful orders\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create a Window Spec\n",
    "window_spec = Window.partitionBy('cust_id').orderBy('order_date')\n",
    "\n",
    "# filter the successful orders\n",
    "filtered_orders_df = orders_df.filter(col('order_status') == 'Y')\n",
    "\n",
    "# add row_number col\n",
    "rn_orders_df = filtered_orders_df.withColumn('next_odr_date', lead('order_date').over(window_spec))\n",
    "rn_orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2437d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+------------+-------------+---------+\n",
      "|cust_id|order_id|order_date|order_status|next_odr_date|retention|\n",
      "+-------+--------+----------+------------+-------------+---------+\n",
      "|      1|     101|2023-01-01|           Y|   2023-01-25|       24|\n",
      "|      1|     102|2023-01-25|           Y|   2023-03-01|       35|\n",
      "|      1|     103|2023-03-01|           Y|         NULL|     NULL|\n",
      "|      2|     201|2023-01-10|           Y|   2023-02-25|       46|\n",
      "|      2|     203|2023-02-25|           Y|         NULL|     NULL|\n",
      "|      3|     301|2023-02-01|           Y|   2023-02-25|       24|\n",
      "|      3|     302|2023-02-25|           Y|         NULL|     NULL|\n",
      "+-------+--------+----------+------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retention -> diff between order_date and next_order_date\n",
    "retention_check_df = rn_orders_df.withColumn('retention', datediff(col('next_odr_date'),col('order_date')))\n",
    "retention_check_df.show()\n",
    "# result\n",
    "# result = retention_check_df.filter('retention ==  1').select('cust_id').distinct()\n",
    "\n",
    "# result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc416f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadf6f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab030cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff2aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Manual)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
